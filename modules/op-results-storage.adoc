// This module is included in the following assembly:
//
// * records/using-tekton-results-for-openshift-pipelines-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="results-storage_{context}"]
= Preparing storage or LokiStack forwarding for logging information

{tekton-results} uses separate storage for logging information related to pipeline runs and task runs. You can configure any one of the following types of storage:

* Persistent volume claim (PVC) on your {product-title} cluster
* Google Cloud Storage
* S3 bucket storage

Alternatively, you can install LokiStack and OpenShift Logging on your {OCP} cluster and configure forwarding of the logging information to LokiStack. This option provides better scalability for higher loads.

[NOTE]
====
In OpenShift Pipelines 1.16, the {tekton-results} ability to natively store logging information on a PVC, in Google Cloud Storage, and in S3 bucket storage is deprecated and is planned to be removed in a future release.
====

Logging information is available using the {tekton-results} command-line interface and API, irrespective of the type of logging information storage or LokiStack forwarding that you configure.

.Procedure

Complete one of the following procedures:

* To use a PVC, complete the following steps:
.. Create a file named `pvc.yaml` with the following definition for the PVC:
+
[source,yaml]
----
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: tekton-logs
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
----
.. Apply the definition by entering the following command:
+
[source,terminal]
----
$ oc apply -n openshift-pipelines -f pvc.yaml
----

* To use Google Cloud Storage, complete the following steps:
.. Create an application credentials file by using the `gcloud` command. For instructions about providing application credentials in a file, see link:https://cloud.google.com/docs/authentication/application-default-credentials#personal[User credentials provided by using the gcloud CLI] in the Google Cloud documentation.
.. Create a secret from the application credentials file by entering the following command:
+
[source,terminal]
----
$ oc create secret generic gcs-credentials \
  --from-file=$HOME/.config/gcloud/application_default_credentials.json \
  -n openshift-pipelines
----
+
Adjust the path and filename of the application credentials file as necessary.

* To use S3 bucket storage, complete the following steps:
.. Create a file named `s3_secret.yaml` with the following content:
+
[source,yaml]
----
  apiVersion: v1
  kind: Secret
  metadata:
    name: my_custom_secret
    namespace: tekton-pipelines
  type: Opaque
  stringData:
    S3_BUCKET_NAME: bucket1 # <1>
    S3_ENDPOINT: https://example.localhost.com # <2>
    S3_HOSTNAME_IMMUTABLE: "false"
    S3_REGION: region-1 # <3>
    S3_ACCESS_KEY_ID: "1234" # <4>
    S3_SECRET_ACCESS_KEY: secret_key # <5>
    S3_MULTI_PART_SIZE: "5242880"
----
<1> The name of the S3 storage bucket
<2> The S3 API endpoint URL
<3> The S3 region
<4> The S3 access key ID
<5> The S3 secret access key

.. Create a secret from the file by entering the following command:
+
[source,terminal]
----
$ oc create secret generic s3-credentials \
  --from-file=s3_secret.yaml -n openshift-pipelines
----

* To configure LokiStack forwarding, complete the following steps:

.. On your {OCP} cluster, install LokiStack by using the Loki Operator and also install the OpenShift Logging Operator.

.. Create a `ClusterLogForwarder.yaml` manifest file for the `ClusterLogForwarder` custom resource (CR) with one of the following YAML manifests, dependings on whether you installed OpenShift Logging version 6 or version 5:
+
.YAML manifest for the `ClusterLogForwarder` CR if you installed OpenShift Logging version 6
[source,yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: collector
  namespace: openshift-logging
spec:
  inputs:
  - application:
      selector:
        matchLabels:
          app.kubernetes.io/managed-by: tekton-pipelines
    name: only-tekton
    type: application
  managementState: Managed
  outputs:
  - lokiStack:
      labelKeys:
        application:
          ignoreGlobal: true
          labelKeys:
          - log_type
          - kubernetes.namespace_name
          - openshift_cluster_id
      authentication:
        token:
          from: serviceAccount
      target:
        name: logging-loki
        namespace: openshift-logging
    name: default-lokistack
    tls:
      ca:
        configMapName: openshift-service-ca.crt
        key: service-ca.crt
    type: lokiStack
  pipelines:
  - inputRefs:
    - only-tekton
    name: default-logstore
    outputRefs:
    - default-lokistack
  serviceAccount:
    name: collector
----
+
.YAML manifest for the `ClusterLogForwarder` CR if you installed OpenShift Logging version 5
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  inputs:
  - name: only-tekton
    application:
      selector:
        matchLabels:
          app.kubernetes.io/managed-by: tekton-pipelines
  pipelines:
    - name: enable-default-log-store
      inputRefs: [ only-tekton ]
      outputRefs: [ default ]
----

.. To create the `ClusterLogForwarder` CR in the `openshift-logging` namespace, log into your {OCP} cluster with the {oc-first} as a cluster administrator user, and then enter the following command:
+
[source, terminal]
----
$ oc apply -n openshift-logging ClusterLogForwarder.yaml
----
